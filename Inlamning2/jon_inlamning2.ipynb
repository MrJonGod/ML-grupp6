{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Machine Learning Techniques - Assignment 2\n",
    "\n",
    "## üìù Assignment Overview\n",
    "This notebook is part of **Assignment 2** for the course *Machine Learning Techniques*.  \n",
    "The objective is to build a **text classification model** for Swedish text, using machine learning techniques.  \n",
    "\n",
    "We are provided with preprocessed text data, and our task is to:\n",
    "1. **Select and train multiple classifiers** (e.g., Logistic Regression, SVC, Random Forest, Na√Øve Bayes).\n",
    "2. **Optimize hyperparameters** using **GridSearchCV** to improve model performance.\n",
    "3. **Evaluate the models** using accuracy, precision, recall, and F1-score.\n",
    "4. **Compare models** to determine the best-performing approach.\n",
    "\n",
    "The final goal is to achieve the highest possible accuracy on the test dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Notebook Structure\n",
    "\n",
    "üîπ **1. Data Preprocessing**  \n",
    "    - Load the dataset  \n",
    "    - Perform text cleaning (lowercasing, removing stopwords, stemming)  \n",
    "    - Convert text to numerical features using **TF-IDF vectorization**  \n",
    "\n",
    "üîπ **2. Model Selection & Hyperparameter Optimization**  \n",
    "    - Define multiple classifiers  \n",
    "    - Tune hyperparameters using **GridSearchCV**  \n",
    "\n",
    "üîπ **3. Training & Evaluation of Optimized Models**  \n",
    "    - Train models using the best hyperparameters  \n",
    "    - Evaluate performance using **classification reports**  \n",
    "\n",
    "üîπ **4. Model Comparison & Final Results**  \n",
    "    - Compare model performance  \n",
    "    - Identify the best-performing model  \n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Expected Outcome\n",
    "At the end of this notebook, we will have:\n",
    "‚úÖ A trained text classification model with optimized hyperparameters  \n",
    "‚úÖ Performance evaluation metrics (accuracy, precision, recall, F1-score)  \n",
    "‚úÖ Insights into which model performed best for this dataset  \n",
    "\n",
    "By following this structured approach, we ensure a **systematic and well-documented** machine learning workflow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing regular expressions for text manipulation and cleaning\n",
    "import re\n",
    "# Importing system-specific parameters and functions (e.g., to suppress warnings)\n",
    "import sys\n",
    "# Importing module to control warning messages\n",
    "import warnings\n",
    "# Importing the Natural Language Toolkit for text preprocessing\n",
    "import nltk\n",
    "# Importing pandas for handling and manipulating structured datasets\n",
    "import pandas as pd\n",
    "# Importing numpy for numerical computations\n",
    "import numpy as np\n",
    "# Importing stopword lists for removing common, uninformative words\n",
    "from nltk.corpus import stopwords\n",
    "# Importing SnowballStemmer for reducing words to their root forms (stemming)\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Importing train_test_split for splitting the dataset into training and testing subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Importing TfidfVectorizer for converting text data into numerical vectors using TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\JonGo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specifying the custom path for nltk data to ensure that required resources can be loaded correctly\n",
    "nltk.data.path.append('C:\\\\Users\\\\JonGo/nltk_data')\n",
    "\n",
    "# Downloading the 'punkt_tab' resource, which is used for tokenization purposes\n",
    "# Note: This ensures that nltk can properly split text into sentences or words during preprocessing\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppressing all warning messages to ensure a clean and readable output during execution\n",
    "# This is useful in cases where warnings are not critical to the functioning of the code\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Data Preprocessing\n",
    "\n",
    "In this section, we clean and prepare the raw text data to ensure it is in an optimal format for machine learning.  \n",
    "The preprocessing steps include:\n",
    "- **Text Normalization**: Converting text to lowercase and removing punctuation, digits, and HTML tags.\n",
    "- **Stopword Removal**: Eliminating common words that do not add meaningful information.\n",
    "- **Stemming**: Reducing words to their root form to improve generalization.\n",
    "- **TF-IDF Vectorization**: Converting text data into numerical features for model training.\n",
    "\n",
    "This preprocessing pipeline enhances model accuracy by reducing noise and improving text representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JonGo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load the data from a CSV file located at the specified path\n",
    "# This dataset is the basis for the text classification task\n",
    "data_path = \"C:\\\\workspace\\\\ML\\\\ML-grupp6\\\\Inlamning2\\\\Book1.csv\"\n",
    "data_raw = pd.read_csv(data_path)\n",
    "\n",
    "# Randomly shuffle the data to ensure no unintended patterns influence the model's training\n",
    "data_raw = data_raw.sample(frac=1)\n",
    "\n",
    "# Identify the columns representing the target categories for classification\n",
    "# Here, categories are assumed to start from the 3rd column onwards\n",
    "categories = list(data_raw.columns.values)\n",
    "categories = categories[2:]\n",
    "\n",
    "# Perform basic cleaning on the \"Heading\" column:\n",
    "# - Convert text to lowercase\n",
    "# - Remove punctuation using regular expressions\n",
    "# - Remove numeric digits\n",
    "# - Remove any HTML tags\n",
    "data_raw['Heading'] = (\n",
    "    data_raw['Heading']\n",
    "    .str.lower()\n",
    "    .str.replace('[^\\w\\s]', '', regex=True)  # Remove punctuation\n",
    "    .str.replace('\\d+', '', regex=True)      # Remove digits\n",
    "    .str.replace('<.*?>', '', regex=True)    # Remove HTML tags\n",
    ")\n",
    "\n",
    "# Download and use the Swedish stopwords for further text cleaning\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('swedish'))\n",
    "\n",
    "# Function to remove stopwords from a sentence\n",
    "# This step eliminates common words (e.g., \"och\", \"att\") that are unlikely to contribute to classification\n",
    "def removeStopWords(sentence):\n",
    "    return \" \".join(\n",
    "        [word for word in nltk.word_tokenize(sentence) \n",
    "         if word not in stop_words]\n",
    "    )\n",
    "\n",
    "# Apply the stopword removal function to the \"Heading\" column\n",
    "data_raw['Heading'] = data_raw['Heading'].apply(removeStopWords)\n",
    "\n",
    "# Initialize the Snowball stemmer for Swedish\n",
    "# This reduces words to their root form (e.g., \"springer\" -> \"spring\")\n",
    "stemmer = SnowballStemmer(\"swedish\")\n",
    "\n",
    "# Function to apply stemming to each sentence\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stemSentence += stemmer.stem(word) + \" \"\n",
    "    return stemSentence.strip()\n",
    "\n",
    "# Apply stemming to the \"Heading\" column\n",
    "data_raw['Heading'] = data_raw['Heading'].apply(stemming)\n",
    "\n",
    "# Split the dataset into training and testing subsets\n",
    "# - 70% of the data is used for training\n",
    "# - 30% is used for testing\n",
    "train, test = train_test_split(data_raw, random_state=42, test_size=0.30, shuffle=True)\n",
    "\n",
    "# Extract the text (features) for training and testing\n",
    "train_text = train['Heading']\n",
    "test_text = test['Heading']\n",
    "\n",
    "# Use TF-IDF vectorization to convert text data into numerical feature vectors\n",
    "# - `strip_accents='unicode'` removes accents for better normalization\n",
    "# - `analyzer='word'` specifies word-based tokenization\n",
    "# - `ngram_range=(1,3)` considers unigrams, bigrams, and trigrams\n",
    "# - `norm='l2'` normalizes feature vectors\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', \n",
    "                             analyzer='word', \n",
    "                             ngram_range=(1,3), \n",
    "                             norm='l2')\n",
    "vectorizer.fit(train_text)\n",
    "\n",
    "# Transform the training and testing text data into numerical feature matrices\n",
    "x_train = vectorizer.transform(train_text)\n",
    "y_train = train.drop(labels=['Id', 'Heading'], axis=1)  # Drop non-target columns for the labels\n",
    "\n",
    "x_test = vectorizer.transform(test_text)\n",
    "y_test = test.drop(labels=['Id', 'Heading'], axis=1)  # Drop non-target columns for the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LogisticRegression for implementing a linear classification model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Importing MultinomialNB, a Naive Bayes classifier suitable for discrete data (e.g., text)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Importing RandomForestClassifier, an ensemble learning method based on decision trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Importing OneVsRestClassifier for handling multi-label classification problems\n",
    "# This wraps around base classifiers and trains one model per label\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Importing MultiOutputClassifier to handle multi-output/multi-label classification problems\n",
    "# Useful for models like RandomForest which don't natively support multi-label classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# Importing metrics for evaluating classification models:\n",
    "# - accuracy_score: calculates the proportion of correct predictions\n",
    "# - classification_report: provides a detailed evaluation with precision, recall, and F1-score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Importing GridSearchCV for hyperparameter tuning to find the best model configuration\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Importing SVC (Support Vector Classifier), a versatile classifier capable of handling both linear and non-linear problems\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ Model Selection & Hyperparameter Optimization\n",
    "\n",
    "To achieve the best classification performance, we experiment with different models:  \n",
    "- **Logistic Regression**\n",
    "- **Support Vector Classifier (SVC)**\n",
    "- **Random Forest**\n",
    "- **Multinomial Na√Øve Bayes**\n",
    "\n",
    "Each model is fine-tuned using **GridSearchCV**, which performs cross-validation over a range of hyperparameters.  \n",
    "This ensures that we identify the best model configuration for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids for each model\n",
    "# These grids specify the range of values to test during hyperparameter tuning with GridSearchCV\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        'estimator__C': [0.1, 1, 10],  # Regularization strength (higher values = less regularization)\n",
    "        'estimator__penalty': ['l1', 'l2']  # Type of regularization (L1: Lasso, L2: Ridge)\n",
    "    },\n",
    "    \"SVC\": {\n",
    "        'estimator__C': [0.1, 1, 10],  # Regularization parameter (controls margin width)\n",
    "        'estimator__kernel': ['linear', 'rbf'],  # Type of kernel (linear or radial basis function)\n",
    "        'estimator__gamma': [0.001, 0.01, 0.1]  # Kernel coefficient for non-linear kernels\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'estimator__n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "        'estimator__max_depth': [None, 10, 20],  # Maximum depth of each tree (None = no limit)\n",
    "        'estimator__min_samples_split': [2, 5, 10]  # Minimum samples required to split a node\n",
    "    },\n",
    "    \"MultinomialNB\": {\n",
    "        'estimator__alpha': [0.1, 0.5, 1.0, 2.0],  # Laplace smoothing parameter\n",
    "        'estimator__fit_prior': [True, False]  # Whether to use class prior probabilities\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define models for classification\n",
    "# Each model is wrapped in OneVsRestClassifier to handle multi-label classification\n",
    "models = {\n",
    "    \"Logistic Regression\": OneVsRestClassifier(LogisticRegression(solver='liblinear')),\n",
    "    \"SVC\": OneVsRestClassifier(SVC()),\n",
    "    \"Random Forest\": OneVsRestClassifier(RandomForestClassifier()),\n",
    "    \"MultinomialNB\": OneVsRestClassifier(MultinomialNB())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Training & Evaluation of Optimized Models\n",
    "\n",
    "Once the best hyperparameters are determined, we train each model on the preprocessed dataset.  \n",
    "For evaluation, we compute:\n",
    "- **Accuracy**: Overall correctness of predictions.\n",
    "- **Precision & Recall**: How well each class is predicted.\n",
    "- **F1-score**: A balance between precision and recall.\n",
    "\n",
    "By comparing these metrics across models, we can select the best-performing classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing Logistic Regression...\n",
      "Best params for Logistic Regression: {'estimator__C': 10, 'estimator__penalty': 'l1'}\n",
      "Best score for Logistic Regression: 0.2565208796639486\n",
      "\n",
      "Test accuracy for Logistic Regression: 0.3442622950819672\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Politik       0.66      0.60      0.63       136\n",
      "        Utbildning       0.37      0.35      0.36        20\n",
      "          Religion       0.50      0.20      0.29         5\n",
      "             Miljo       0.50      0.25      0.33        36\n",
      "           Ekonomi       0.68      0.56      0.62       158\n",
      "     LivsstilFritt       0.65      0.38      0.48        81\n",
      "SamhalleKonflikter       0.68      0.56      0.61       197\n",
      "             Halsa       0.67      0.37      0.47        82\n",
      "            Idrott       0.59      0.37      0.45        52\n",
      "   VetenskapTeknik       0.21      0.16      0.18        19\n",
      "\n",
      "         micro avg       0.64      0.49      0.55       786\n",
      "         macro avg       0.55      0.38      0.44       786\n",
      "      weighted avg       0.64      0.49      0.55       786\n",
      "       samples avg       0.51      0.52      0.49       786\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Optimizing SVC...\n",
      "Best params for SVC: {'estimator__C': 10, 'estimator__gamma': 0.001, 'estimator__kernel': 'linear'}\n",
      "Best score for SVC: 0.22489498393872004\n",
      "\n",
      "Test accuracy for SVC: 0.3819672131147541\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Politik       0.70      0.54      0.61       136\n",
      "        Utbildning       0.50      0.15      0.23        20\n",
      "          Religion       1.00      0.20      0.33         5\n",
      "             Miljo       0.67      0.17      0.27        36\n",
      "           Ekonomi       0.73      0.56      0.64       158\n",
      "     LivsstilFritt       0.93      0.32      0.48        81\n",
      "SamhalleKonflikter       0.69      0.61      0.65       197\n",
      "             Halsa       0.79      0.33      0.47        82\n",
      "            Idrott       0.94      0.29      0.44        52\n",
      "   VetenskapTeknik       0.67      0.21      0.32        19\n",
      "\n",
      "         micro avg       0.73      0.46      0.57       786\n",
      "         macro avg       0.76      0.34      0.44       786\n",
      "      weighted avg       0.75      0.46      0.55       786\n",
      "       samples avg       0.52      0.50      0.49       786\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Optimizing Random Forest...\n",
      "Best params for Random Forest: {'estimator__max_depth': None, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 200}\n",
      "Best score for Random Forest: 0.15813689152458613\n",
      "\n",
      "Test accuracy for Random Forest: 0.21475409836065573\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Politik       0.73      0.38      0.50       136\n",
      "        Utbildning       0.67      0.10      0.17        20\n",
      "          Religion       0.00      0.00      0.00         5\n",
      "             Miljo       0.83      0.14      0.24        36\n",
      "           Ekonomi       0.75      0.34      0.46       158\n",
      "     LivsstilFritt       0.92      0.14      0.24        81\n",
      "SamhalleKonflikter       0.76      0.33      0.46       197\n",
      "             Halsa       0.89      0.21      0.34        82\n",
      "            Idrott       1.00      0.04      0.07        52\n",
      "   VetenskapTeknik       0.67      0.11      0.18        19\n",
      "\n",
      "         micro avg       0.76      0.26      0.39       786\n",
      "         macro avg       0.72      0.18      0.27       786\n",
      "      weighted avg       0.79      0.26      0.38       786\n",
      "       samples avg       0.29      0.28      0.28       786\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Optimizing MultinomialNB...\n",
      "Best params for MultinomialNB: {'estimator__alpha': 0.1, 'estimator__fit_prior': True}\n",
      "Best score for MultinomialNB: 0.2262960217445021\n",
      "\n",
      "Test accuracy for MultinomialNB: 0.3950819672131147\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Politik       0.69      0.48      0.57       136\n",
      "        Utbildning       0.50      0.10      0.17        20\n",
      "          Religion       0.00      0.00      0.00         5\n",
      "             Miljo       0.67      0.17      0.27        36\n",
      "           Ekonomi       0.75      0.58      0.65       158\n",
      "     LivsstilFritt       0.91      0.36      0.51        81\n",
      "SamhalleKonflikter       0.73      0.61      0.67       197\n",
      "             Halsa       0.82      0.34      0.48        82\n",
      "            Idrott       0.88      0.27      0.41        52\n",
      "   VetenskapTeknik       0.57      0.21      0.31        19\n",
      "\n",
      "         micro avg       0.75      0.46      0.57       786\n",
      "         macro avg       0.65      0.31      0.40       786\n",
      "      weighted avg       0.75      0.46      0.55       786\n",
      "       samples avg       0.52      0.50      0.50       786\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Iterate through each model and its corresponding hyperparameter grid\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Optimizing {model_name}...\")\n",
    "    \n",
    "    # Initialize GridSearchCV for hyperparameter tuning\n",
    "    # - model: the classifier to optimize\n",
    "    # - param_grids[model_name]: the hyperparameter grid for the specific model\n",
    "    # - cv=5: 5-fold cross-validation\n",
    "    # - scoring='accuracy': use accuracy as the evaluation metric\n",
    "    # - n_jobs=-1: use all available processors for parallel processing\n",
    "    grid = GridSearchCV(model, param_grids[model_name], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    grid.fit(x_train, y_train)\n",
    "    \n",
    "    # Print the best hyperparameters and the corresponding cross-validation score\n",
    "    print(f\"Best params for {model_name}: {grid.best_params_}\")\n",
    "    print(f\"Best score for {model_name}: {grid.best_score_}\\n\")\n",
    "    \n",
    "    # Evaluate the best model on the test data\n",
    "    best_clf = grid.best_estimator_  # Retrieve the best estimator found by GridSearchCV\n",
    "    y_pred = best_clf.predict(x_test)  # Make predictions on the test set\n",
    "    \n",
    "    # Print the test accuracy and a detailed classification report\n",
    "    print(f\"Test accuracy for {model_name}: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=categories))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Model Comparison & Final Results\n",
    "\n",
    "The final step involves comparing the accuracy of each model to determine the best-performing classifier.  \n",
    "The results include:\n",
    "- **Best hyperparameters for each model**\n",
    "- **Classification reports with precision, recall, and F1-score**\n",
    "- **A summary of which model achieved the highest accuracy**\n",
    "\n",
    "This analysis helps us make an informed decision on the most suitable model for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and optimizing LogReg...\n",
      "LogReg Accuracy: 0.3360655737704918\n",
      "Best parameters for LogReg: {'estimator__C': 10, 'estimator__penalty': 'l1'}\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Politik       0.66      0.60      0.63       136\n",
      "        Utbildning       0.41      0.35      0.38        20\n",
      "          Religion       0.50      0.20      0.29         5\n",
      "             Miljo       0.50      0.25      0.33        36\n",
      "           Ekonomi       0.67      0.55      0.61       158\n",
      "     LivsstilFritt       0.64      0.37      0.47        81\n",
      "SamhalleKonflikter       0.68      0.56      0.62       197\n",
      "             Halsa       0.66      0.35      0.46        82\n",
      "            Idrott       0.58      0.35      0.43        52\n",
      "   VetenskapTeknik       0.21      0.16      0.18        19\n",
      "\n",
      "         micro avg       0.64      0.48      0.55       786\n",
      "         macro avg       0.55      0.37      0.44       786\n",
      "      weighted avg       0.63      0.48      0.54       786\n",
      "       samples avg       0.50      0.51      0.49       786\n",
      "\n",
      "------------------------------------------------------------\n",
      "Training and optimizing SVC...\n",
      "SVC Accuracy: 0.3819672131147541\n",
      "Best parameters for SVC: {'estimator__C': 10, 'estimator__gamma': 0.001, 'estimator__kernel': 'linear'}\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Politik       0.70      0.54      0.61       136\n",
      "        Utbildning       0.50      0.15      0.23        20\n",
      "          Religion       1.00      0.20      0.33         5\n",
      "             Miljo       0.67      0.17      0.27        36\n",
      "           Ekonomi       0.73      0.56      0.64       158\n",
      "     LivsstilFritt       0.93      0.32      0.48        81\n",
      "SamhalleKonflikter       0.69      0.61      0.65       197\n",
      "             Halsa       0.79      0.33      0.47        82\n",
      "            Idrott       0.94      0.29      0.44        52\n",
      "   VetenskapTeknik       0.67      0.21      0.32        19\n",
      "\n",
      "         micro avg       0.73      0.46      0.57       786\n",
      "         macro avg       0.76      0.34      0.44       786\n",
      "      weighted avg       0.75      0.46      0.55       786\n",
      "       samples avg       0.52      0.50      0.49       786\n",
      "\n",
      "------------------------------------------------------------\n",
      "Training and optimizing NaiveBayes...\n",
      "NaiveBayes Accuracy: 0.3950819672131147\n",
      "Best parameters for NaiveBayes: {'estimator__alpha': 0.1, 'estimator__fit_prior': True}\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Politik       0.69      0.48      0.57       136\n",
      "        Utbildning       0.50      0.10      0.17        20\n",
      "          Religion       0.00      0.00      0.00         5\n",
      "             Miljo       0.67      0.17      0.27        36\n",
      "           Ekonomi       0.75      0.58      0.65       158\n",
      "     LivsstilFritt       0.91      0.36      0.51        81\n",
      "SamhalleKonflikter       0.73      0.61      0.67       197\n",
      "             Halsa       0.82      0.34      0.48        82\n",
      "            Idrott       0.88      0.27      0.41        52\n",
      "   VetenskapTeknik       0.57      0.21      0.31        19\n",
      "\n",
      "         micro avg       0.75      0.46      0.57       786\n",
      "         macro avg       0.65      0.31      0.40       786\n",
      "      weighted avg       0.75      0.46      0.55       786\n",
      "       samples avg       0.52      0.50      0.50       786\n",
      "\n",
      "------------------------------------------------------------\n",
      "Training and optimizing RandomForest...\n",
      "RandomForest Accuracy: 0.22131147540983606\n",
      "Best parameters for RandomForest: {'estimator__max_depth': None, 'estimator__min_samples_split': 10, 'estimator__n_estimators': 50}\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Politik       0.78      0.38      0.51       136\n",
      "        Utbildning       0.67      0.10      0.17        20\n",
      "          Religion       0.00      0.00      0.00         5\n",
      "             Miljo       0.83      0.14      0.24        36\n",
      "           Ekonomi       0.74      0.34      0.46       158\n",
      "     LivsstilFritt       0.92      0.15      0.26        81\n",
      "SamhalleKonflikter       0.76      0.35      0.47       197\n",
      "             Halsa       0.90      0.22      0.35        82\n",
      "            Idrott       1.00      0.04      0.07        52\n",
      "   VetenskapTeknik       0.67      0.11      0.18        19\n",
      "\n",
      "         micro avg       0.78      0.27      0.40       786\n",
      "         macro avg       0.73      0.18      0.27       786\n",
      "      weighted avg       0.80      0.27      0.39       786\n",
      "       samples avg       0.30      0.29      0.29       786\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary of optimized classifiers using GridSearchCV\n",
    "# Each model is paired with its corresponding hyperparameter grid for tuning\n",
    "optimized_classifiers = {\n",
    "    \"LogReg\": GridSearchCV(\n",
    "        OneVsRestClassifier(LogisticRegression(solver='liblinear')),  # Logistic Regression for multi-label classification\n",
    "        param_grids[\"Logistic Regression\"],  # Hyperparameter grid for Logistic Regression\n",
    "        cv=5,  # Perform 5-fold cross-validation\n",
    "        scoring=\"accuracy\",  # Use accuracy as the evaluation metric\n",
    "        n_jobs=-1  # Utilize all available processors for parallel processing\n",
    "    ),\n",
    "    \"SVC\": GridSearchCV(\n",
    "        OneVsRestClassifier(SVC()),  # Support Vector Classifier for multi-label classification\n",
    "        param_grids[\"SVC\"],  # Hyperparameter grid for SVC\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"NaiveBayes\": GridSearchCV(\n",
    "        OneVsRestClassifier(MultinomialNB()),  # Naive Bayes classifier for multi-label classification\n",
    "        param_grids[\"MultinomialNB\"],  # Hyperparameter grid for Naive Bayes\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"RandomForest\": GridSearchCV(\n",
    "        OneVsRestClassifier(RandomForestClassifier()),  # Random Forest for multi-label classification\n",
    "        param_grids[\"Random Forest\"],  # Hyperparameter grid for Random Forest\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate the optimized models\n",
    "for name, grid in optimized_classifiers.items():\n",
    "    print(f\"Training and optimizing {name}...\")\n",
    "    \n",
    "    # Fit the model using GridSearchCV on the training data\n",
    "    grid.fit(x_train, y_train)\n",
    "    \n",
    "    # Retrieve the best model found during hyperparameter tuning\n",
    "    best_model = grid.best_estimator_\n",
    "    \n",
    "    # Use the best model to make predictions on the test data\n",
    "    y_pred = best_model.predict(x_test)\n",
    "    \n",
    "    # Evaluate the model's performance on the test data\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
    "    print(f\"{name} Accuracy: {accuracy}\")  # Print accuracy score\n",
    "    print(f\"Best parameters for {name}: {grid.best_params_}\")  # Print the best hyperparameters\n",
    "    print(classification_report(y_test, y_pred, target_names=categories))  # Print detailed classification metrics\n",
    "    print(\"-\" * 60)  # Separator for better readability in the output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
